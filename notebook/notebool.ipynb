{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 640, 3)\n",
      "(640, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "\n",
    "img1 = cv2.imread(\"../static_files/test_image.jpg\")\n",
    "print(img1.shape)\n",
    "\n",
    "img2 = Image.open(\"../static_files/test_image.jpg\")  \n",
    "img2 = np.asarray(img2)\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_RGB2BGR)\n",
    "print(img2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to /home/caveman/.cache/torch/hub/master.zip\n",
      "YOLOv5 ðŸš€ 2022-12-15 Python-3.10.6 torch-1.11.0+cu102 CPU\n",
      "\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "YOLOv5s summary: 214 layers, 7235389 parameters, 7235389 gradients\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=False, force_reload=True, autoshape=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '.yolov5s_v2.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('.yolov5s_v2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classes = [3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YOLOv5 <class 'models.common.Detections'> instance\n",
       "image 1/1: 640x640 4 motorcycles\n",
       "Speed: 6.5ms pre-process, 379.6ms inference, 3.3ms NMS per image at shape (1, 3, 640, 640)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepsparse.pipelines.custom_pipeline import CustomTaskPipeline\n",
    "from deepsparse import Pipeline\n",
    "import json,os\n",
    "\n",
    "static_file_path= '../static_files/yolo_classes.json'\n",
    "try:\n",
    "    with open(static_file_path, 'r') as fp:\n",
    "        key_to_string = json.load(fp)\n",
    "    string_to_key = {value:key for key, value in key_to_string.items()}\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Error {e} while loading model class\")\n",
    "\n",
    "kwargs = {}\n",
    "# ***************************** initialize YOLO-V5 **********************************\n",
    "# self.detector = torch.load(kwargs.get('weights','yolov5/weights/yolov5s.pt'), map_location=self.device)['model'].float()  # load to FP32\n",
    "model_size = kwargs.get('model_size','yolov5-s') #['yolov5-s', 'yolov5-l']\n",
    "model_weight_path = kwargs.get('model_weight_path', \"./model_weight/\"+model_size+\"_v2.onnx\") #'yolov5/weights/yolov5s.pt') \"./model_weight/yolov5s.pt\"\n",
    "pretrained_model = False if os.path.exists(model_weight_path) else True\n",
    "stub = {\n",
    "            \"yolov5-l\":\"zoo:cv/detection/yolov5-l/pytorch/ultralytics/coco/pruned_quant-aggressive_95\",\n",
    "            \"yolov5-s\":\"zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned_quant-aggressive_94\"\n",
    "        }\n",
    "\n",
    "yolo_pipeline = Pipeline.create(\n",
    "task=\"yolo\",\n",
    "model_path= stub.get(model_size, None) if pretrained_model else model_weight_path,\n",
    "class_names=list(key_to_string.keys()),   # if using custom model, pass in a list of classes the model will clasify or a path to a json file containing them\n",
    "model_config=None,  # if using custom model, pass in the path to a local model config file here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"model_weight\"):\n",
    "    os.makedirs(\"model_weight\")\n",
    "\n",
    "if os.path.isfile(yolo_pipeline.onnx_file_path):\n",
    "    os.rename(yolo_pipeline.onnx_file_path, model_weight_path) if pretrained_model else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./model_weight/yolov5-s_v2.onnx'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yolo_pipeline.onnx_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "3 validation errors for YOLOOutput\nboxes\n  field required (type=value_error.missing)\nscores\n  field required (type=value_error.missing)\nlabels\n  field required (type=value_error.missing)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/caveman/Documents/python_dev/object_tracking_yolov5_deepsort/notebook/notebool.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/caveman/Documents/python_dev/object_tracking_yolov5_deepsort/notebook/notebool.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m yolo_pipeline\u001b[39m.\u001b[39;49moutput_schema()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/object_tracking_yolov5_deepsort-RND3WqFJ/lib/python3.10/site-packages/pydantic/main.py:342\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 3 validation errors for YOLOOutput\nboxes\n  field required (type=value_error.missing)\nscores\n  field required (type=value_error.missing)\nlabels\n  field required (type=value_error.missing)"
     ]
    }
   ],
   "source": [
    "yolo_pipeline.output_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "image = cv2.imread(\"../static_files/test_image.jpg\")\n",
    "image.astype(np.uint8)\n",
    "print(image.shape)\n",
    "# yolo_pipeline.class_names([2,3])\n",
    "pipeline_outputs = yolo_pipeline(images=image, iou_thres=0.6, conf_thres=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YOLOOutput(boxes=[[[45.29338073730469, 168.49630737304688, 228.07969665527344, 311.3185119628906], [76.69711303710938, 32.82611083984375, 199.724365234375, 131.873291015625], [63.94458770751953, 485.4278564453125, 213.818115234375, 621.5233154296875], [240.37136840820312, 541.4109497070312, 394.8977966308594, 614.7621459960938], [252.99996948242188, 40.96147155761719, 404.4091491699219, 129.52249145507812], [421.9398498535156, 201.61544799804688, 581.795166015625, 300.8406066894531], [430.152099609375, 519.798583984375, 578.988525390625, 620.963623046875], [75.14987182617188, 325.3128662109375, 195.07424926757812, 470.36883544921875], [460.8572082519531, 352.4356689453125, 578.5552978515625, 451.70166015625], [426.97796630859375, 15.077621459960938, 588.6062622070312, 147.41343688964844], [245.1121826171875, 197.54347229003906, 390.9814453125, 287.2884521484375], [242.71847534179688, 342.217529296875, 400.8564147949219, 463.16705322265625]]], scores=[[0.934207558631897, 0.9156649708747864, 0.9097804427146912, 0.90463787317276, 0.8891618251800537, 0.8874203562736511, 0.8869575262069702, 0.8805232048034668, 0.8648037910461426, 0.8235767483711243, 0.7745827436447144, 0.7658988237380981]], labels=[['3', '3', '3', '2', '2', '7', '7', '3', '7', '7', '2', '2']])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "object_tracking_yolov5_deepsort-RND3WqFJ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8477147905689faec5a2518b4111d350571af744bf9bd8a13c4f3a0f2cc376e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
